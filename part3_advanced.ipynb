{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Analysis\n",
    "\n",
    "In this part, we will implement advanced analysis techniques for physiological time series data, including time-domain feature extraction, frequency analysis, and wavelet transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "import pywt\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Time-Domain Feature Extraction\n",
    "\n",
    "Implement the `extract_time_domain_features` function to extract various time-domain features from physiological signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_domain_features(data, window_size=60):\n",
    "    \"\"\"Extract time-domain features from physiological signals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Input data with columns: ['timestamp', 'heart_rate', 'eda', 'temperature', 'subject_id', 'session']\n",
    "    window_size : int, optional\n",
    "        Size of the rolling window in seconds, default=60\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing extracted features for each window\n",
    "    \"\"\"\n",
    "    # Convert window_size from seconds to number of samples\n",
    "    # Assuming data is sampled at 1 Hz (1 sample per second)\n",
    "    window_samples = window_size\n",
    "    \n",
    "    # Initialize DataFrame for features\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Basic statistics using rolling window\n",
    "    features['mean'] = data['heart_rate'].rolling(window=window_samples).mean()\n",
    "    features['std'] = data['heart_rate'].rolling(window=window_samples).std()\n",
    "    features['min'] = data['heart_rate'].rolling(window=window_samples).min()\n",
    "    features['max'] = data['heart_rate'].rolling(window=window_samples).max()\n",
    "    \n",
    "    # Heart rate statistics\n",
    "    features['mean_hr'] = data['heart_rate'].rolling(window=window_samples).mean()\n",
    "    features['std_hr'] = data['heart_rate'].rolling(window=window_samples).std()\n",
    "    \n",
    "    # Beat-to-beat variability\n",
    "    rr_intervals = 60000 / data['heart_rate']  # Convert HR to RR intervals in ms\n",
    "    \n",
    "    # Calculate successive differences within each window\n",
    "    rr_diff = rr_intervals.diff()\n",
    "    \n",
    "    # RMSSD (Root Mean Square of Successive Differences)\n",
    "    features['rmssd'] = np.sqrt(rr_diff.rolling(window=window_samples).apply(lambda x: np.mean(x**2)))\n",
    "    \n",
    "    # SDNN (Standard Deviation of NN intervals)\n",
    "    features['sdnn'] = rr_intervals.rolling(window=window_samples).std()\n",
    "    \n",
    "    # pNN50 (Percentage of successive RR intervals differing by >50ms)\n",
    "    features['pnn50'] = rr_diff.rolling(window=window_samples).apply(\n",
    "        lambda x: 100 * np.sum(np.abs(x) > 50) / len(x) if len(x) > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Drop NaN values from rolling window calculations\n",
    "    features = features.dropna()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Frequency Analysis\n",
    "\n",
    "Implement the `analyze_frequency_components` function to perform frequency-domain analysis on the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_frequency_components(data, sampling_rate, window_size=60):\n",
    "    \"\"\"Perform frequency-domain analysis on physiological signals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Input data with columns: ['timestamp', 'heart_rate', 'eda', 'temperature', 'subject_id', 'session']\n",
    "    sampling_rate : float\n",
    "        Sampling rate of the signal in Hz\n",
    "    window_size : int, optional\n",
    "        Size of the analysis window in seconds, default=60\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing frequency analysis results\n",
    "    \"\"\"\n",
    "    # Convert window_size from seconds to number of samples\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Process data in windows\n",
    "    n_windows = len(data) // window_samples\n",
    "    all_frequencies = []\n",
    "    all_power = []\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        window_data = data['heart_rate'].iloc[i*window_samples:(i+1)*window_samples]\n",
    "        \n",
    "        # Calculate PSD using Welch's method\n",
    "        frequencies, power = signal.welch(\n",
    "            window_data,\n",
    "            fs=sampling_rate,\n",
    "            nperseg=window_samples\n",
    "        )\n",
    "        \n",
    "        all_frequencies.append(frequencies)\n",
    "        all_power.append(power)\n",
    "    \n",
    "    # Average results across windows\n",
    "    results['frequencies'] = np.mean(all_frequencies, axis=0)\n",
    "    results['power'] = np.mean(all_power, axis=0)\n",
    "    \n",
    "    # Define frequency bands\n",
    "    bands = {\n",
    "        'VLF': (0.003, 0.04),\n",
    "        'LF': (0.04, 0.15),\n",
    "        'HF': (0.15, 0.4)\n",
    "    }\n",
    "    \n",
    "    # Calculate power in each band\n",
    "    results['bands'] = {}\n",
    "    for band_name, (low, high) in bands.items():\n",
    "        mask = (results['frequencies'] >= low) & (results['frequencies'] <= high)\n",
    "        results['bands'][band_name] = np.sum(results['power'][mask])\n",
    "    \n",
    "    # Calculate LF/HF ratio\n",
    "    results['bands']['LF/HF'] = results['bands']['LF'] / results['bands']['HF']\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time-Frequency Analysis\n",
    "\n",
    "Implement the `analyze_time_frequency_features` function to analyze time-frequency features using wavelet transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_time_frequency_features(data, sampling_rate, window_size=60):\n",
    "    \"\"\"Analyze time-frequency features using wavelet transforms.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Input data with columns: ['timestamp', 'heart_rate', 'eda', 'temperature', 'subject_id', 'session']\n",
    "    sampling_rate : float\n",
    "        Sampling rate of the signal in Hz\n",
    "    window_size : int, optional\n",
    "        Size of the analysis window in seconds, default=60\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing time-frequency analysis results\n",
    "    \"\"\"\n",
    "    # Convert window_size from seconds to number of samples\n",
    "    window_samples = int(window_size * sampling_rate)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Define wavelet scales\n",
    "    scales = np.arange(1, 128)\n",
    "    results['scales'] = scales\n",
    "    \n",
    "    # Process data in windows\n",
    "    n_windows = len(data) // window_samples\n",
    "    all_coefficients = []\n",
    "    all_energy = []\n",
    "    \n",
    "    for i in range(n_windows):\n",
    "        window_data = data['heart_rate'].iloc[i*window_samples:(i+1)*window_samples]\n",
    "        \n",
    "        # Apply continuous wavelet transform\n",
    "        coefficients, frequencies = pywt.cwt(\n",
    "            window_data,\n",
    "            scales,\n",
    "            'morl',\n",
    "            sampling_period=1/sampling_rate\n",
    "        )\n",
    "        \n",
    "        all_coefficients.append(coefficients)\n",
    "        all_energy.append(np.abs(coefficients)**2)\n",
    "    \n",
    "    # Average results across windows\n",
    "    results['coefficients'] = np.mean(all_coefficients, axis=0)\n",
    "    results['time_frequency_energy'] = np.mean(all_energy, axis=0)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Here's how to use these functions with your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing S1, Final\n",
      "Processing S1, Midterm 1\n",
      "Processing S1, Midterm 2\n",
      "Processing S10, Final\n",
      "Processing S10, Midterm 1\n",
      "Processing S10, Midterm 2\n",
      "Processing S2, Final\n",
      "Processing S2, Midterm 1\n",
      "Processing S2, Midterm 2\n",
      "Processing S3, Final\n",
      "Processing S3, Midterm 1\n",
      "Processing S3, Midterm 2\n",
      "Processing S4, Final\n",
      "Processing S4, Midterm 1\n",
      "Processing S4, Midterm 2\n",
      "Processing S5, Final\n",
      "Processing S5, Midterm 1\n",
      "Processing S5, Midterm 2\n",
      "Processing S6, Final\n",
      "Processing S6, Midterm 1\n",
      "Processing S6, Midterm 2\n",
      "Processing S7, Final\n",
      "Processing S7, Midterm 1\n",
      "Processing S7, Midterm 2\n",
      "Processing S8, Final\n",
      "Processing S8, Midterm 1\n",
      "Processing S8, Midterm 2\n",
      "Processing S9, Final\n",
      "Processing S9, Midterm 1\n",
      "Processing S9, Midterm 2\n"
     ]
    }
   ],
   "source": [
    "# First, load the processed data\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load all processed data files\n",
    "files = glob.glob('data/processed/*_processed.csv')\n",
    "dfs = [pd.read_csv(f) for f in files]\n",
    "processed_data = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "processed_data['timestamp'] = pd.to_datetime(processed_data['timestamp'])\n",
    "\n",
    "# Get unique subjects and sessions\n",
    "subjects = processed_data['subject_id'].unique()\n",
    "sessions = processed_data['session'].unique()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('data/advanced_analysis', exist_ok=True)\n",
    "\n",
    "# Process each subject and session\n",
    "for subject in subjects:\n",
    "    for session in sessions:\n",
    "        print(f\"Processing {subject}, {session}\")  # Optional: to track progress\n",
    "        \n",
    "        # Filter data for this subject/session\n",
    "        mask = (processed_data['subject_id'] == subject) & (processed_data['session'] == session)\n",
    "        subject_data = processed_data[mask]\n",
    "        \n",
    "        if len(subject_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # 1. Extract time domain features\n",
    "        time_features = extract_time_domain_features(subject_data, window_size=60)\n",
    "        time_features.to_csv(f'data/advanced_analysis/{subject}_{session}_time_features.csv')\n",
    "        \n",
    "        # 2. Analyze frequency components\n",
    "        sampling_rate = 4.0  # Hz\n",
    "        freq_results = analyze_frequency_components(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_fft.npy', freq_results)\n",
    "        \n",
    "        # 3. Analyze time-frequency features\n",
    "        tf_results = analyze_time_frequency_features(subject_data, sampling_rate, window_size=60)\n",
    "        np.save(f'data/advanced_analysis/{subject}_{session}_wavelet.npy', tf_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
